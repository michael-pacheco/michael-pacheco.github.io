{% extends "base.html" %}
{% block body %}
<link rel="stylesheet" type="text/css" href="{{url_for('static', filename='css/default.css')}}">
<div style="height:85px; width: 100%; background: white;"></div>
  <div class="main" style="">
    <div class="container">
        <div class="row">
            <!-- Blog Post Content Column -->
            <div class="col-lg-8">
                <!-- Blog Post -->
                <!-- Title -->
                <h1>Reinforcement Learning: Grid World</h1>
                <!-- Author -->
                <p class="lead">
                    by <a href="#">Michael</a>
                </p>
                <hr>
                <!-- Date/Time -->
                <p><span class="glyphicon glyphicon-time"></span>Posted on {{time_created}}</p>
                <hr>
                <!-- Preview Image -->
                <img style="width: 50%" src="/static/blog/RL-grid-world/header.jpg">


                  <p>
                    Grid world is another common problem to solve in the world of reinforcement learning. There are many different versions of it, but the goal is essentially the same: to get to the goal space.
                    The agent is in a 2D world, where it must navigate to find the exit space. With reinforcment learning, we can allow our agent to play optimally.
                    This problem is very applicable to reinforcement learning because it is a Markov Deicision Process - a fancy way of saying we can abstract the environment in a certain way that makes life easy.
                    <br>MDP's are defined by four main attributes:
                    <ol>
                      <li>States</li>
                      <li>Actions</li>
                      <li>Transition probabilities</li>
                      <li>Rewards</li>
                    </ol>
                  </p>


                  <p>
                  The version of grid world that we are looking at is the one in the picture above!
                  In this version, there are walls in the middle of the world. This just means our transition probabilities will be different depending whether or not our agent is beside a wall. With that said, let's start defining this grid world in terms of a MDP:

                  <ul>
                    <li>
                      <ultext>States: </ultext>
                      <p>A list contain the x,y coordinates of our agent: [x,y]</p>
                    </li>
                    <li><ultext>Actions: </ultext>
                      <p>Moving in one of the following four directions: up, down, left, or right.</p>
                    </li>
                    <li><ultext>Transition probabilities: </ultext>
                      <p>Agent will move in the inteded direction with probability of p1.</p>
                        <p>Agent will not change its position with probability of p2.</p>
                      <p>  Agent will move to an adjacent state with probability (1-p1-p2)/2 (eg. if the agent moves up, with probability (1-p1-p2)/2 it will move left or right)
                      </p>
                      <p>
                        All actions that would cause the agent to step out of the grid world or bump into a wall will not move the agent with probability p1+2, or cause the agent to move to an adjacent state with probability (1-p1-p2)/2.

                      </p>
                    </li>
                    <li><ultext>Rewards: </ultext>
                      <p>(-1) for every state that is not the terminal/goal state, and (+500) for reaching the terminal state.</p>
                    </li>
                  </ul>

                </p>

                <p>
                  With all of these transition rules, defining the environment is rather tedious, as there are a lot of if statements.
                   The code for the environment can be found in the project <a target="_blank" href="http://github.com/michael-pacheco/RL-grid-world">on my GitHub</a>.
                </p>

                <p>
                  The algorithms I'll be going through are two called Q-Learning and SARSA. Both are similar - their main difference is in their value update function. Let's take a quick look at them.
                </p>
                <p>Q Learning:
                  <img style="width: 70%" src="/static/blog/RL-grid-world/q.jpg">
                </p>
                <p>
                  SARSA:
                  <img style="width: 70%" src="/static/blog/RL-grid-world/sarsa.jpg">
                </p>
                <br>

                <p> In both of these descriptions of the algorithms, 't' refers to a time step. As a result, we are updating a value function for the State (St) the agent is in and Action (At) the agent chose in that state at time step 't'. The variable 't+1' refers to anything in the next time step - so when we refer to Q(St+1, At+1) in SARSA, this means the value estimate for the next state and next action that occurs as a result of taking action At.
                  We refer to Q(St+1, a) in Q-Learning instead because we are always choosing the action with respect to the epsilon greedy policy, therefore variable 'a' refers to the action in state S at time t+1 that has the highest expected value.
                <p> It is important to note that Q-Learning is an off-policy algorithm, while SARSA is an on-policy algorithm. An on-policy algorithm is one that explicitly improves an initially arbitrary policy throughout the process.
                   An off-policy algorithm is one that does not improve a policy over time.

                   Apart from this, the algorithms are quite similar. The difference is that Q-Learning always selects the action that holds the maximum expected return (epsilon greedy), while SARSA will always follow its policy that it's performing policy iteration on.
                   The algorithms are the same in our case because we will be implementing and epsilon greedy policy as our policy for SARSA.
                </p>
                <p>We will also be using a method called exploring starts to help balance exploration and exploitation. Having exploring starts means that whenever we start a new game, our agent has an equal chance to spawn/start in any of the states in the world.
                  The assumption of exploring starts is that our agent will do enough exploring with this method, along with whatever policy we have in place (which is epsilon greedy in our case).
                </p>

                <p>With all of these concepts out of the way, all we have left is to implement the algorithm!</p>

                <p> Here is the code for the Q-Learning algorithm: </p>

                {{q|safe}}

                <br>
                <p> And the code for the SARSA algorithm: </p>

                {{sarsa|safe}}


                <p>
                  As you can see, the main differences in the algorithms is that there is an arbitrary policy defined in SARSA (line 15), and there is not any policy defined in the Q-Learning algorithm. As a result of this, I implement the epsilon greedy algorithm directly into the Q-Learning algorithm.
                  The rest of the code is mainly for setting up this algorithm; finding the what state and action we are performing, the resulting state and action we end up in after performing our original action, and updating our value function.
                </p>
                <p>Also, you may have noticed that the value of epsilon slowly decreases over time. The philosophy behind this is that at the start of our learning we should be exploring more frequently, as we do not know the optimal action. As our agent continues to learn and go through iterations, it does not need to explore as much, and should focus on exploiting more. If we slowly decrease epsilon over time, the amount of exploring our agent will do will decrease, which should increase our maximum return even more.
                </p>
                <p>To compare the two algorithms I run the algorithms with their "default" epsilon. gamma, alpha, p1 and p2 values - which can be seen in the screenshots. Here is a screenshot after running both algorithms for 10,000 iterations: </p>
                <img style="width: 100%" src="/static/blog/RL-grid-world/comparison.jpg">
                <br>
                <p>The policies that they produce are both nearly optimal. There are a few states here and there that do not yield the optimal action -
                  but recall that we are using the exploring starts method, thus this is the result of the agent failing to reach that state often enough to determine the optimal action.</p>

                  <p>Even though these seem like fairly decent algorithms (which they are), there are ones that are even better that are improvements of these two (including Double-Q-Learning and Expected SARSA).
                  </p>
              <br><br>
              <p>
                You can find the entire project which includes all the files, as well as other algorithms I've implemented (including Monte Carlo, Double-Q-Learning and Expected SARSA) <a target="_blank" href="http://github.com/michael-pacheco/RL-grid-world">on my GitHub</a>.
              </p>


              <hr>
              <p><strong>SHARE ON</strong></p>

              <a target="_blank" href="https://twitter.com/intent/tweet?text=http://michaelpacheco.net/blog/RL-grid-world" class="btn btn-twitter" style="background: #55acee;">
                  <i class="fa fa-twitter"></i>
                  Tweet
              </a>
              <a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://michaelpacheco.net/blog/RL-grid-world" class="btn btn-facebook" style="background: #3b5998;">
                  <i class="fa fa-facebook-square"></i>
                  Share
              </a>
              <a target="_blank" href="https://plus.google.com/share?url=http://michaelpacheco.net/blog/RL-grid-world" class="btn btn-google-plus" style="background: #dd4b39;">
                  <i class="fa fa-google-plus"></i>
                  Share
              </a>
              <a target="_blank" href="http://github.com/michael-pacheco/RL-grid-world" class="btn btn-github" style="background: #333333;">
                  <i class="fa fa-github"></i>
                Star
              </a>

            </div>

            <!-- Blog Sidebar Widgets Column -->
            <div class="col-md-4">
              <br>
              <!-- Blog Search Well -->
              <div class="well">
                  <h4>Blog Search</h4>
                  <form class="input-group" method="POST">
                    {{form.csrf_token}}
                      {{form.search(class="form-control")}}
                      <span class="input-group-btn">
                        <button type="submit" id="submit" class="btn btn-primary">
                              <span class="glyphicon glyphicon-search">Search</span>
                      </button>
                      </span>
                  </form>
                  <!-- /.input-group -->
              </div>

                <!-- Related Blog Posts Well -->
                <div class="well">
                    <h4>Related Blog Posts</h4>
                    <div class="row">
                      <div class="col-lg-6">
                        <ul class="list-unstyled">
                          {% for post in related_blog_posts %}
                            <li><a href="{{post["url"]}}">{{post["title"]}}</a></li>
                            <hr class="styled">
                            {% if loop.index % 2 == 0 %}
                              </ul>
                            </div>
                            <div class="col-lg-6">
                              <ul class="list-unstyled">
                            {% endif %}
                          {% endfor %}

                            </ul>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>

                <!-- Other Blog Posts Well -->
                <div class="well">
                    <h4>Other Blog Posts</h4>
                    <div class="row">
                      <div class="col-lg-6">
                        <ul class="list-unstyled">
                          {% for post in other_blog_posts %}
                            <li><a href="{{post["url"]}}">{{post["title"]}}</a></li>
                            <hr class="styled">
                            {% if loop.index % 2 == 0 %}
                              </ul>
                            </div>
                            <div class="col-lg-6">
                              <ul class="list-unstyled">
                            {% endif %}
                          {% endfor %}

                            </ul>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>

                <!-- Side Widget Well -->


            </div>

        </div>
        <!-- /.row -->

        <hr>


    </div>
  </div>

<style>
ultext {
  text-decoration: underline;
}

img {
  margin: auto;display: block;
}

.highlight pre {width: 44%;  overflow-x: scroll; display:block; overflow:auto;}
.linenos {vertical-align: top;}

custompre {
  padding: 4px;
margin: 0 0 10px;
font-size: 13px;
line-height: 1.42857143;
word-break: break-all;
word-wrap: break-word;
color: #333;
background-color: #f5f5f5;
border: 1.5px solid #ccc;
border-radius: 4px;
font-family: Menlo,Monaco,Consolas,"Courier New",monospace;
}
</style>
{% endblock %}
