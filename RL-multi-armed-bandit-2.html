{% extends "base.html" %}
{% block body %}
<link rel="stylesheet" type="text/css" href="{{url_for('static', filename='css/default.css')}}">
<div style="height:85px; width: 100%; background: white;"></div>
  <div class="main" style="">
    <div class="container">
        <div class="row">
            <!-- Blog Post Content Column -->
            <div class="col-lg-8">
                <!-- Blog Post -->
                <!-- Title -->
                <h1>Reinforcement Learning: Multi-Armed Bandit - Part II</h1>
                <!-- Author -->
                <p class="lead">
                    by <a href="#">Michael</a>
                </p>
                <hr>
                <!-- Date/Time -->
                <p><span class="glyphicon glyphicon-time"></span>Posted on {{time_created}}</p>
                <hr>
                <!-- Preview Image -->
                <img src="/static/blog/RL-multi-armed-bandit-2/header.jpg" style="width: 50%;"><br>
                <br>
                  <p>This is part two of tackling the <a target="_blank" href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit</a> problem!
                    If you haven't seen part one of solving this problem, I strongly encourage you to check it out! <a href="{{SERVER_IP}}/blog/RL-multi-armed-bandit">Here is the link to part one!</a>
                  </p>

                  <p>
                    The next two reinforcement learning algoritms I'll be looking at are known better as stochastic learning automata.
                    They were originally described as finite state automata, but were later more commonly referred to in the form of algorithms (as description by a finite state automaton is inconvinient).
                  </p>
                  <p>
                    These two algorithms are known as <ultext>Linear Reward Penalty</ultext> and <ultext>Linear Reward Inaction</ultext>.
                    Their names are directly related to their algorithm, and pretty much fully describes what they do!
                    Both of them will choose actions according to a probability distribution, meaning the sum of the different probabilities of pulling each lever add up to 1.
                    Their updates to these probabilities will also keep the sum of them equal to one.
                  </p>
                  <p>

                    The way these algorithms works depends directly on the response from the environment after performing an action.
                    That is, depending on if the agent performs an action and the environment does or doesn't return a reward, the algorithm will perform differently.
                  </p>
                  <p>They also use learning parameters, denoted as alpha and beta, that affect the amount of increasing and decreasing of probabilities in the algorithms.
                    The learning paramters alpha (&alpha;) and beta (&beta;) can both be any value between 0 and 1. For Linear Reward Inaction, it is common for alpha to be fairly small,
                       as this gives the agent a better chance to not choose the wrong actions.

                  </p>
                  <br>
                  <p>Let p<sub>i</sub> denote the probability of performing action i.</p>
                  <p>
                  <ultext>Linear Reward Penalty</ultext>
                </p>
                  <p>After performing action action p<sub>i</sub>:
                    <br>
                     If action p<sub>i</sub> results in a reward:</p>
                  <p>
                    p<sub>i</sub> = p<sub>i</sub> + &alpha;(1 - p<sub>i</sub>) //increases the probability of choosing action p<sub>i</sub>.
                    <br>
                    p<sub>j</sub> = (1 - &alpha;) * p<sub>j</sub> //for all actions that aren't the one the agent performed, decrease their probabilities of being chosen.
                  </p>
                  <p>If action p<sub>i</sub> does not result in a reward:</p>
                  <p>
                    p<sub>j</sub> = (&beta;)/(k-1) + (1 - &beta;) * p<sub>j</sub> //for all actions that aren't the one the agent performed, increase their probabilities of being chosen.
                    p<sub>i</sub> = (1 - &beta;) * p<sub>i</sub> //decrease the probabilty of choosing the action p<sub>i</sub>
                  </p>
                  <p>
                    <ultext>Linear Reward Inaction</ultext>
                    <p>After performing action action p<sub>i</sub>:
                      <br>
                       If action p<sub>i</sub> results in a reward:</p>
                    <p>
                      p<sub>i</sub> = p<sub>i</sub> + &alpha;(1 - p<sub>i</sub>) //increases the probability of choosing action p<sub>i</sub>.
                      <br>
                      p<sub>j</sub> = (1 - &alpha;) * p<sub>j</sub> //for all actions that aren't the one the agent performed, decrease their probabilities of being chosen.
                    </p>
                    <br>
                    <p>As you can see, Linear Reward Inaction does not do anything if the reward signal received after performing action p<sub>i</sub> is 0 - it is fairly optimistic, as it only updates when it receives a reward!
                    </p>

                    <p>Here is the program containing the implementation for both algorithms:</p>

                    {{code|safe}}
                    <br>
                    <p>
                      I have them both set to run 10 times for 10,000 iterations. At the end of each run, the program will print out the agent's average reward for that run.
                    </p>
                    <p>
                      Here is the output of the program:
                    </p>
                    <br>
                    <img src="/static/blog/RL-multi-armed-bandit-2/pic.jpg">
                    <br>
                    <p>
                      It seems Linear Reward Inaction performs better than Linear Reward Penalty on average (and also the original algorithm from <a href="{{SERVER_IP}}/blog/RL-multi-armed-bandit">part one</a>).
                      Let's see what happens if we combine Linear Reward Penalty with our original algorithm from <a href="{{SERVER_IP}}/blog/RL-multi-armed-bandit">part one</a>.<br>
                      We will change the Linear Reward Penalty algorithm to choose an action greedily (choose the lever with the highest chance of being chosen), instead of choosing randomly according to the probability distribution.
                    </p>
                    <p>
                      We change the way our agent chooses an action to: <pre>lever = lever_probability.index(max(lever_probability))</pre>
                    </p>
                    <br>
                    <img src="/static/blog/RL-multi-armed-bandit-2/pic2.jpg">
                    <br>
                    <p>
                      That was a simple modification that seemed to improve the algorithm tremendously!
                      Unfortunately, we can't do the same modification to Linear Reward Inaction, as it would result in the algorithm choosing one action exclusively for the whole run.
                    </p>
                    <p>
                      Comparing these two algorithms to the original one in <a href="{{SERVER_IP}}/blog/RL-multi-armed-bandit">part one</a>, they're both much better.
                      Of course, there are even better algorithms and more modification that could be made to improve these!
                    </p>


              <br><br>
              <p>
                You can find the entire project which includes all the files <a target="_blank" href="http://github.com/michael-pacheco/RL-multi-armed-bandit">on my GitHub</a>
              </p>


              <hr>
              <p><strong>SHARE ON</strong></p>

              <a target="_blank" href="https://twitter.com/intent/tweet?text=http://michaelpacheco.net/blog/RL-multi-armed-bandit-2" class="btn btn-twitter" style="background: #55acee;">
                  <i class="fa fa-twitter"></i>
                  Tweet
              </a>
              <a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://michaelpacheco.net/blog/RL-multi-armed-bandit-2" class="btn btn-facebook" style="background: #3b5998;">
                  <i class="fa fa-facebook-square"></i>
                  Share
              </a>
              <a target="_blank" href="https://plus.google.com/share?url=http://michaelpacheco.net/blog/RL-multi-armed-bandit-2" class="btn btn-google-plus" style="background: #dd4b39;">
                  <i class="fa fa-google-plus"></i>
                  Share
              </a>
              <a target="_blank" href="http://github.com/michael-pacheco/RL-multi-armed-bandit" class="btn btn-github" style="background: #333333;">
                  <i class="fa fa-github"></i>
                Star
              </a>

            </div>

            <!-- Blog Sidebar Widgets Column -->
            <div class="col-md-4">
              <br>
                <!-- Blog Search Well -->
                <div class="well">
                    <h4>Blog Search</h4>
                    <form class="input-group" method="POST">
                      {{form.csrf_token}}
                        {{form.search(class="form-control")}}
                        <span class="input-group-btn">
                          <button type="submit" id="submit" class="btn btn-primary">
                                <span class="glyphicon glyphicon-search">Search</span>
                        </button>
                        </span>
                    </form>
                    <!-- /.input-group -->
                </div>

                <!-- Related Blog Posts Well -->
                <br>
                <div class="well">
                    <h4>Related Blog Posts</h4>
                    <div class="row">
                      <div class="col-lg-6">
                        <ul class="list-unstyled">
                          {% for post in related_blog_posts %}
                            <li><a href="{{post["url"]}}">{{post["title"]}}</a></li>
                            <hr class="styled">
                            {% if loop.index % 2 == 0 %}
                              </ul>
                            </div>
                            <div class="col-lg-6">
                              <ul class="list-unstyled">
                            {% endif %}
                          {% endfor %}

                            </ul>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>
                <!-- Side Widget Well -->
                <!-- Other Blog Posts Well -->
                <div class="well">
                    <h4>Other Blog Posts</h4>
                    <div class="row">
                      <div class="col-lg-6">
                        <ul class="list-unstyled">
                          {% for post in other_blog_posts %}
                            <li><a href="{{post["url"]}}">{{post["title"]}}</a></li>
                            <hr class="styled">
                            {% if loop.index % 2 == 0 %}
                              </ul>
                            </div>
                            <div class="col-lg-6">
                              <ul class="list-unstyled">
                            {% endif %}
                          {% endfor %}

                            </ul>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>
            </div>
        </div>
        <hr>
    </div>
  </div>
</div>


<style>
ultext {
  text-decoration: underline;
}

img {
  margin: auto;display: block;
}

.highlight pre {width: 57%; overflow-x: scroll;}
.linenos {vertical-align: top;}

custompre {
padding: 4px;
margin: 0 0 10px;
font-size: 13px;
line-height: 1.42857143;
overflow-x: scroll;
color: #333;
background-color: #f5f5f5;
border: 1.5px solid #ccc;
border-radius: 4px;
font-family: Menlo,Monaco,Consolas,"Courier New",monospace;
}
</style>
{% endblock %}
