{% extends "base.html" %}
{% block body %}
<link rel="stylesheet" type="text/css" href="{{url_for('static', filename='css/default.css')}}">
<div style="height:85px; width: 100%; background: white;"></div>
  <div class="main" style="">
    <div class="container">
        <div class="row">
            <!-- Blog Post Content Column -->
            <div class="col-lg-8">
                <!-- Blog Post -->
                <!-- Title -->
                <h1>Reinforcement Learning: Multi-Armed Bandit</h1>
                <!-- Author -->
                <p class="lead">
                    by <a href="#">Michael</a>
                </p>
                <hr>
                <!-- Date/Time -->
                <p><span class="glyphicon glyphicon-time"></span>Posted on {{time_created}}</p>
                <hr>
                <!-- Preview Image -->
                <img src="/static/blog/RL-multi-armed-bandit/header.jpg" style="width: 50%;"><br>

                  <p>Reinforcement learning is a field in artificial intelligence that is a type of machine learning that is neither supervised nor unsupervised.
                  It consists of a programmed agent whose goal is to maximize its cumulative reward over time. How the environment (and as a result the reward system) is defined is up to the programmer, though it should be a replica of the actual environment.
                </p>
                  <p>
                  With the recent advances in AI including neural networks, the learning capability of deep learning agents seem to be unstoppable. This method of machine learning is applicable to many games in general, which a lot of reinforcement learning enthusiasts are interested in (including me!). There have been several reports on deep learning agents playing and successfully beating various <a target="_blank" href="https://www.atari.com/">Atari</a> games. Here is a YouTube video showing a deep learning agent's capability:
                </p>
                <br>
                  <iframe width="560" height="315" style="margin: 0 auto; display: block;" src="https://www.youtube.com/embed/V1eYniJ0Rnk" frameborder="0" allowfullscreen></iframe>
                  <br><br><br>
                  <p>
                  I always enjoy comparing this concept to training a dog. To train your dog, you give it treats when it rolls over as a reward of positive reinforcement. Because all dogs love treats, it will recognize that if it performs that specific action it will get one. As a result, it will learn to keep repetitively doing the action if you say the magic words. In conclusion, a reinforcement learning agent is really no different than a dog (<strong>yes</strong> it is just as cute as one too). It's also extremely satisfying to see your little baby agent learn to become a full grown (somtimes) optimally acting adult agent as you watch it continue to learn.
                </p>
                <p>
                  One well known and rather trivial problem in reinforcement learning is the <a target="_blank" href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit problem</a>. A one-armed bandit is essentially a slot machine with one lever to pull. Following, a k-armed bandit is one with k-levers to pull. The problem proposes that the arms of a k-armed bandit all have different probabilities (random or stochastic) of giving the player a reward when being pulled. The goal is to design an agent that will find the arm that has the highest probability of reward, and once this arm is discovered, the agent will pull that one the most to maximize its reward. The problem within the problem, and also one of the concepts always contemplated in every RL problem, is how to balance exploration and exploitation. Exploration is when the agent performs an action that is not the one believed to be optimal (or the greedy action), while exploitation is when the agent performs the believed optimal (greedy) action. The goal is to balance these, as the agent must explore all actions to find the best one, but it must also perform the greedy action the most so that it can maximize its reward.
                </p>
                <p>
                  We will be using a very simple algorithm for updating the values of each action:
                </p>
                  <p>
                    <pre> expected_return[lever] = expected_return[lever] + (1/lever_count[lever])* (instant_reward - expected_return[lever]) </pre>
                  </p>
                  <p>
                  We are taking the previous value of the action and adding it to the sum of the previous reward (0 or 1), subtracted by the previous value of the action, divided by the number of times the agent pulled that particular lever. (Where lever is the action, expected_return[lever] is the value of the current action, and reward_count is a list that holds the total reward for each individual action)
                </p>
                <p>
                  Let's begin by mapping out a 10-armed bandit environment. To do this, we'll first make a list with the names of the levers (q1 to q10). For each of these levers, we will also initialize a percentage randomly that will dictate the lever's threshold for giving a reward. We also initialize variables leverCount, rewardCount and greedyCount, which will hold the amount of times each lever is pulled, the total reward each lever has returned, and the current expected return value for each lever respectively. Now, I have my epsilon value set to 0.33. Whenever the randomizer is below this threshold, our agent will perform an exploring action. You can change the epsilon value to whatever you'd like, and should experiment to find which one yields the maximum expected return in the end <custom style="font-size: 50%">(I like my value, find your own!)</custom>. Finally, for tracking purposes, we find the lever that yields the highest return, so that while the agent is going through iterations, we can later see which lever is truly the most optimal whether or not the agent is exploiting that action.
                </p>
                {{breakdown1|safe}}
                <p>
                  Now that our environment is setup, we can begin describing how our agent will perform! We have to define the cases for exploration and exploitation: we randomly generate a number - if it's less than epsilon we will explore. If not, we will exploit the action with the highest expected return value. Next, we see if we get a reward from pulling the lever - which will occur if a randomly generated number is less than the chance for the lever we pulled to give a reward. Finally, we increment how many times we pulled that lever, and update our expected value of that lever using our expected value function. Every 100 iterations, we print what the optimal action is, its reward chance, and how many times our agent pulled it. After the 1000th iteration, the program will output all the values used.
                </p>
                {{breakdown2|safe}}
                <br>
                <p>

                  Here is the output of a single run of the program:
                </p>
                <br>
                  <img src="{{url_for('static', filename='blog/RL-multi-armed-bandit/pic1.jpg')}}">
                  <br>
                  <p>

                  Unsatisfactory. Let's try again:
                </p>
                <br>
                  <img src="{{url_for('static', filename='blog/RL-multi-armed-bandit/pic2.jpg')}}">
                  <br>
                  <p>
                  As you can see, our agent doesn't always perform like me in my high school gym class (I was pretty good at hockey for your information). It seems to struggle to pull the truly optimal lever in some runs. The factors affecting our possiblity to pull the best lever include: the epsilon value, the differences between the lever's percentage to return a reward, and our update function.
                  The epsilon value matters because we might be exploring too much or too little (and consequently exploiting too little or too much), which will directly affect our cumulative rewards.
                </p><p>The difference between the lever's higher percentages may be too small, causing our agent to believe one lever is the optimal one when it is not.
                  For example, if there are two levers: one with 99% chance of returning a reward, and another with 90%, and the agent chooses the one with 90% first, it will believe that one is the most optimal. The difference between 99% and 90% somewhat negligible to the agent - a 90% chance to receive a reward is still extremely high.

                </p>
                <p>
                  Both of these problems deal with exploration vs. exploitation, but our final problem does not - the update function. Our function is very simple - for example,there are no weights, no penalties for receiving no return - and so on - all of which can help our agent out.
                </p>
                <p>
                  That being said, there are a lot of ways that we can improve this algorithm, and even other algorithms we can use instead. I'll be looking at two other algorithms for this problem in <a href="/blog/RL-multi-armed-bandit-2">part two of this tutorial!</a>
                </p>
              <br><br>
              <p>
                You can find the entire project which includes all the files <a target="_blank" href="http://github.com/michael-pacheco/RL-multi-armed-bandit">on my GitHub</a>
              </p>


              <hr>
              <p><strong>SHARE ON</strong></p>

              <a target="_blank" href="https://twitter.com/intent/tweet?text=http://michaelpacheco.net/blog/RL-multi-armed-bandit" class="btn btn-twitter" style="background: #55acee;">
                  <i class="fa fa-twitter"></i>
                  Tweet
              </a>
              <a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://michaelpacheco.net/blog/RL-multi-armed-bandit" class="btn btn-facebook" style="background: #3b5998;">
                  <i class="fa fa-facebook-square"></i>
                  Share
              </a>
              <a target="_blank" href="https://plus.google.com/share?url=http://michaelpacheco.net/blog/RL-multi-armed-bandit" class="btn btn-google-plus" style="background: #dd4b39;">
                  <i class="fa fa-google-plus"></i>
                  Share
              </a>
              <a target="_blank" href="http://github.com/michael-pacheco/RL-multi-armed-bandit" class="btn btn-github" style="background: #333333;">
                  <i class="fa fa-github"></i>
                Star
              </a>

            </div>

            <!-- Blog Sidebar Widgets Column -->
            <div class="col-md-4">
              <br>
                <!-- Blog Search Well -->
                <div class="well">
                    <h4>Blog Search</h4>
                    <form class="input-group" method="POST">
                      {{form.csrf_token}}
                        {{form.search(class="form-control")}}
                        <span class="input-group-btn">
                          <button type="submit" id="submit" class="btn btn-primary">
                                <span class="glyphicon glyphicon-search">Search</span>
                        </button>
                        </span>
                    </form>
                    <!-- /.input-group -->
                </div>

                <!-- Related Blog Posts Well -->
                <br>
                <div class="well">
                    <h4>Related Blog Posts</h4>
                    <div class="row">
                      <div class="col-lg-6">
                        <ul class="list-unstyled">
                          {% for post in related_blog_posts %}
                            <li><a href="{{post["url"]}}">{{post["title"]}}</a></li>
                            <hr class="styled">
                            {% if loop.index % 2 == 0 %}
                              </ul>
                            </div>
                            <div class="col-lg-6">
                              <ul class="list-unstyled">
                            {% endif %}
                          {% endfor %}

                            </ul>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>
                <!-- Side Widget Well -->
                <!-- Other Blog Posts Well -->
                <div class="well">
                    <h4>Other Blog Posts</h4>
                    <div class="row">
                      <div class="col-lg-6">
                        <ul class="list-unstyled">
                          {% for post in other_blog_posts %}
                            <li><a href="{{post["url"]}}">{{post["title"]}}</a></li>
                            <hr class="styled">
                            {% if loop.index % 2 == 0 %}
                              </ul>
                            </div>
                            <div class="col-lg-6">
                              <ul class="list-unstyled">
                            {% endif %}
                          {% endfor %}

                            </ul>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>
            </div>
        </div>
        <hr>
    </div>
  </div>
</div>


<style>
ultext {
  text-decoration: underline;
}

img {
  margin: auto;display: block;
}

.highlight pre {width: 720px; overflow-x: scroll;}
.linenos {vertical-align: top;}

custompre {
padding: 4px;
margin: 0 0 10px;
font-size: 13px;
line-height: 1.42857143;
overflow-x: scroll;
color: #333;
background-color: #f5f5f5;
border: 1.5px solid #ccc;
border-radius: 4px;
font-family: Menlo,Monaco,Consolas,"Courier New",monospace;
}
</style>
{% endblock %}
